\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}

\title{Comparisons}
\order{2}
\begin{document}


\section{Comparison to Other Pattern Mining Approaches}

The Hyperon Pattern Miner differs from classical pattern mining methods in several key respects.   A full literature review would go far beyond the scope of this tutorial, but we'll take a few. moments to briefly compare the Pattern Miner's design and capabilities to several well-known approaches for mining patterns from structures like relational databases or graphs.   To make things concrete we'll depict each approach reviewed via a crude pseudocode sketch.  Actual implementations are of course often (sometimes tremendously) more complex for scalability reasons.


\subsection{Frequent Itemset and Sequence Mining}
\subsubsection{Apriori}

Apriori \cite{agrawal1994fast} is in many ways the grand-daddy of frequent pattern mining algorithms.  It finds frequent itemsets in transactional data by level-wise candidate generation and pruning.

\begin{verbatim}
Algorithm Apriori(D, min_support):
    L1 = { frequent 1-itemsets in D }
    k = 2
    while L(k-1) not empty:
        Ck = apriori_gen(L(k-1))
        for each transaction t in D:
            for each candidate c in Ck:
                if c subset of t:
                    count[c] += 1
        Lk = { c in Ck | count[c] >= min_support }
        k += 1
    return Union of all Lk
\end{verbatim}

In Apriori, the algorithm first scans the entire dataset once to count each individual item's frequency and collects those that meet the minimum support threshold.  It then enters a loop where, at iteration $k$, it generates candidate $k$-itemsets by joining pairs of frequent $(k-1)$-itemsets that share $(k-2)$ items (the \texttt{apriori\_gen} step).  For each transaction, it tests which candidates are subsets of that transaction and increments their counts.  After scanning all transactions, it discards candidates whose counts fall below \texttt{min\_support}.  This process repeats, increasing $k$, until no new candidates survive, and the union of all frequent itemsets is returned.

\subsubsection{FP-growth}

FP-growth \cite{han2000mining} builds a compact prefix tree and mines it recursively without explicit candidate generation.

\begin{verbatim}
Function FP_Growth(D, min_support):
    tree = build_fp_tree(D, min_support)
    return mine_tree(tree, [])
Procedure mine_tree(tree, prefix):
    for each item i in tree header:
        new_pattern = prefix U {i}
        output new_pattern
        conditional_db = build_conditional_db(tree, i)
        conditional_tree = build_fp_tree(conditional_db, min_support)
        if conditional_tree not empty:
            mine_tree(conditional_tree, new_pattern)
\end{verbatim}

FP-growth begins by constructing an FP-tree: it reads each transaction, orders its frequent items by descending global frequency, and inserts that ordered list as a branch in the tree, sharing common prefixes.  The tree's header table links all occurrences of each item.  To mine the tree, the algorithm processes each item in the header: it appends the item to the current prefix to form a new frequent pattern, builds a conditional pattern base \texttt{(the set of prefix paths leading to that item)}, constructs a conditional FP-tree from that base, and recursively mines the conditional tree.  This avoids the costly candidate generation of Apriori by reusing the tree structure to count pattern occurrences.

\subsubsection{PrefixSpan}

PrefixSpan \cite{pei2001prefixspan} extends sequence mining by recursively projecting databases on discovered prefixes.

\begin{verbatim}
Procedure PrefixSpan(S, prefix, min_support):
    for each item i with support >= min_support in S:
        new_pattern = prefix U {i}
        output new_pattern
        projected_db = project_database(S, i)
        PrefixSpan(projected_db, new_pattern, min_support)
\end{verbatim}

PrefixSpan treats each sequence in the database $s$ as an ordered list of items.  At each call, it finds all items that appear at least \texttt{min\_support} times after the current \texttt{prefix}.  For each such item $i$, it extends the prefix by $i$, outputs the new sequential pattern, then constructs the \emph{projected database} of suffix subsequences that follow each occurrence of $i$ in the original sequences.  The algorithm then recurses on this smaller database.  This pattern-growth approach efficiently enumerates all frequent sequences without generating candidates explicitly.

\subsection{Frequent Subgraph Mining}

\subsubsection{gSpan}

gSpan \cite{yan2002gspan} discovers frequent subgraphs by depth-first search and canonical DFS codes.

\begin{verbatim}
Algorithm gSpan(GraphDB, min_support):
    for each frequent single-edge pattern p:
        DFS_Code = min_dfs_code(p)
        subgraph_mining(p, GraphDB, DFS_Code, min_support)
Procedure subgraph_mining(p, GraphDB, DFS_Code, min_support):
    for each extension e of p:
        if support(e, GraphDB) >= min_support:
            p_new = p extended by e
            DFS_Code_new = update_dfs_code(DFS_Code, e)
            subgraph_mining(p_new, GraphDB, DFS_Code_new, min_support)
\end{verbatim}

gSpan first enumerates all edges that occur in at least \texttt{min\_support} graphs and computes their minimum DFS code--a canonical string representation that uniquely identifies a graph.  In the recursive \texttt{subgraph\_mining} procedure, it attempts to extend the current pattern $p$ by adding one edge \texttt{(and possibly a new node)}, computes the new DFS code for the extended graph, and checks its support across the graph database.  Only extensions meeting the support threshold are pursued further.  By using DFS codes to avoid duplicate subgraph generation, gSpan efficiently explores the lattice of frequent subgraphs.

\subsection{Relational and Inductive Logic Mining}

\subsubsection{WARMR}

WARMR \cite{dehaspe1999frequent} mines frequent relational patterns \texttt{(logical clauses)} by iteratively refining hypotheses.

\begin{verbatim}
Procedure WARMR(min_support):
    C1 = generate_initial_clauses(language_bias)
    k = 1
    while Ck not empty:
        Lk = { c in Ck | support(c) >= min_support }
        C(k+1) = refine_clauses(Lk)
        k += 1
    return Union of all Lk
\end{verbatim}

WARMR operates in the inductive logic programming setting.  It begins by generating a set $C_1$ of all possible single-literal clauses permitted by a user-supplied language bias.  At each iteration $k$, it selects those clauses in $C_k$ whose support across the logical database meets \texttt{min\_support}, calls this $L_k$, then \emph{refines} each clause in $L_k$ by adding one additional literal in all allowed ways to form the next candidate set $C_{k+1}$.  This loop continues until no new clauses survive, and the union of all frequent clauses $L_1 \cup L_2 \cup \dots$ is returned.

\subsubsection{Aleph}

Aleph \cite{srinivasan2002aleph} uses inverse entailment to generate and evaluate Prolog-style clauses, but does not include built-in surprisingness measures.

\begin{verbatim}
Procedure AlephILP(examples, background, settings):
    while not done:
        bottom_clause = derive_bottom_clause(examples, background)
        clause = search_clause_space(bottom_clause, settings)
        evaluate_clause(clause, examples)
        add_best_clause_to_model(clause)
\end{verbatim}

Aleph constructs, for each positive example, a \emph{bottom clause}--the most specific clause that entails that example given the background logic.  It then searches the space of generalizations of this bottom clause \texttt{(by removing or weakening literals)} according to user settings, evaluates each candidate clause against both positive and negative examples, and adds the highest-scoring clause to its hypothesis model.  This process repeats until a stopping criterion is met, producing an inductively learned set of logical rules.

\subsection{How Hyperon Pattern Miner Relates to Prior Approaches}


\begin{itemize}
  \item \textbf{Generalized to Metagraphs}:  
    Apriori and PrefixSpan work on flat tables of transactions or sequences, where each record is an independent list of items or events.  By contrast, Hyperon Pattern Miner operates directly on a single, typed metagraph (the AtomSpace), where nodes can represent entities, concepts or variables, and links can represent arbitrary relations or functions.  This allows the miner to discover patterns that span multiple relation types and that exploit the full connectivity of the data, rather than reducing everything to flat co-occurrence counts.
  \item \textbf{Variable Binding and Logic}:  
    Systems like WARMR and Aleph require you to switch between a logic programming environment (e.g.\ Prolog) and a separate mining or evaluation engine.  Hyperon embeds both pattern generation and logical inference in MeTTa, so that variables, unification and backward-chaining are natively supported.  This seamless integration eliminates the impedance mismatch between mining and reasoning, making it trivial to take a mined pattern and immediately use it as a predicate in probabilistic inference or supervised procedure learning.
  \item \textbf{Greedy Conjunct Expansion}:  
    Subgraph miners like gSpan rely on canonical DFS codes and depth-first search to avoid duplicates, while FP-growth uses complex tree structures to avoid candidate explosion.  Hyperon uses a simple, greedy approach: generate all 1-clause templates, filter by support, then join surviving patterns on shared variables to form 2-clause patterns, and so on.  By leveraging the AtomSpace's optimized match engine and lazy stream evaluation, this method scales without the heavy bookkeeping of DFS codes or prefix trees, and can be tuned easily by adjusting support thresholds at each depth.
  \item \textbf{Single-Graph Mining}:  
    Traditional subgraph miners treat each example graph as a separate transaction and look for subgraphs that appear across many graphs.  In many AGI and knowledge-driven applications, all facts live in one large metagraph rather than multiple snapshots.  Hyperon Pattern Miner treats the entire AtomSpace as a unified database, discovering patterns that cross the boundaries of what would be separate transactions in other systems.  This makes it ideal for mining patterns in evolving knowledge bases, dialogue archives, or combined sensory-action logs.
  \item \textbf{Surprisingness Scoring}:  
    Most classical pattern miners rank patterns purely by support or confidence, which often surfaces obvious or trivial regularities.  Hyperon adds a built-in measure of interestingness, I-Surprisingness, that compares observed co-occurrence to an independence baseline.  By ranking patterns by how much they deviate from random expectation, the miner highlights non-trivial, semantically rich discoveries--patterns that would be lost in a sea of high-support but uninformative itemsets.  Furthermore, the framework is highly customizable, so that users (or AI algorithms) can introduce new measures of interestingness to complement the frequency and the I-Surprisingness measure.
  \item \textbf{Extensibility}:  
    Many mining algorithms are fixed pipelines written in low-level languages, making customization difficult.  Hyperon's entire pipeline lives in MeTTa scripts under \texttt{match/} and \texttt{experiments/}, so you can modify conjunction logic, add dependent-type filters, or plug in new scoring measures without touching C++ or restarting the system.  This flexibility accelerates experimentation and domain adaptation, enabling rapid prototyping of novel pattern-driven learning workflows.
\end{itemize}

\end{document}